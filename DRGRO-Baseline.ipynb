{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNRA/miWu9aNl0PchhxDIfI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4YBLAgF_BFc","executionInfo":{"status":"ok","timestamp":1696355869415,"user_tz":-330,"elapsed":3969,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}},"outputId":"2428c957-f116-4bef-a935-7222b240e293"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: fse in /usr/local/lib/python3.10/dist-packages (1.0.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from fse) (1.23.5)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from fse) (1.11.3)\n","Requirement already satisfied: smart-open>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from fse) (6.4.0)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from fse) (1.2.2)\n","Requirement already satisfied: gensim>=4 in /usr/local/lib/python3.10/dist-packages (from fse) (4.3.2)\n","Requirement already satisfied: wordfreq>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from fse) (3.0.3)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from fse) (0.17.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from fse) (5.9.5)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->fse) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->fse) (3.2.0)\n","Requirement already satisfied: ftfy>=6.1 in /usr/local/lib/python3.10/dist-packages (from wordfreq>=2.2.1->fse) (6.1.1)\n","Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.10/dist-packages (from wordfreq>=2.2.1->fse) (3.3.0)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from wordfreq>=2.2.1->fse) (1.0.7)\n","Requirement already satisfied: regex>=2021.7.6 in /usr/local/lib/python3.10/dist-packages (from wordfreq>=2.2.1->fse) (2023.6.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->fse) (3.12.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->fse) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->fse) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->fse) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->fse) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->fse) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->fse) (23.1)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1->wordfreq>=2.2.1->fse) (0.2.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->fse) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->fse) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->fse) (2.0.5)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->fse) (2023.7.22)\n"]}],"source":["!pip install fse"]},{"cell_type":"code","source":["!wget https://github.com/skoltech-nlp/detox/releases/download/emnlp2021/filtered_paranmt.zip\n","!unzip filtered_paranmt.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dxE3Rat1oqYO","executionInfo":{"status":"ok","timestamp":1696355912700,"user_tz":-330,"elapsed":43289,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}},"outputId":"1d1eb6ce-d63c-49f4-d58a-e6cc050c1dea"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-03 17:57:49--  https://github.com/skoltech-nlp/detox/releases/download/emnlp2021/filtered_paranmt.zip\n","Resolving github.com (github.com)... 192.30.255.112\n","Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://github.com/s-nlp/detox/releases/download/emnlp2021/filtered_paranmt.zip [following]\n","--2023-10-03 17:57:49--  https://github.com/s-nlp/detox/releases/download/emnlp2021/filtered_paranmt.zip\n","Reusing existing connection to github.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/402743074/ea18dc6d-ab2d-49da-9cd3-2903867da5d3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231003%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231003T175749Z&X-Amz-Expires=300&X-Amz-Signature=ae31ae57ff551fe9209a015721a10382dbf62d7f0b665c9687655e144226ece9&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=402743074&response-content-disposition=attachment%3B%20filename%3Dfiltered_paranmt.zip&response-content-type=application%2Foctet-stream [following]\n","--2023-10-03 17:57:49--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/402743074/ea18dc6d-ab2d-49da-9cd3-2903867da5d3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231003%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231003T175749Z&X-Amz-Expires=300&X-Amz-Signature=ae31ae57ff551fe9209a015721a10382dbf62d7f0b665c9687655e144226ece9&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=402743074&response-content-disposition=attachment%3B%20filename%3Dfiltered_paranmt.zip&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 44376072 (42M) [application/octet-stream]\n","Saving to: ‘filtered_paranmt.zip.1’\n","\n","filtered_paranmt.zi 100%[===================>]  42.32M  --.-KB/s    in 0.1s    \n","\n","2023-10-03 17:57:49 (295 MB/s) - ‘filtered_paranmt.zip.1’ saved [44376072/44376072]\n","\n","Archive:  filtered_paranmt.zip\n","replace filtered.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: filtered.tsv            \n"]}]},{"cell_type":"code","source":["!pip install pytorch-nlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H1fJ413TAdqF","executionInfo":{"status":"ok","timestamp":1696355916819,"user_tz":-330,"elapsed":4123,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}},"outputId":"14c2c005-1d35-4b33-cf52-c3a4e897759f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.10/dist-packages (0.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-nlp) (1.23.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-nlp) (4.66.1)\n"]}]},{"cell_type":"code","source":["!pip install pandas\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IgfGe3UIK7LU","executionInfo":{"status":"ok","timestamp":1696355923720,"user_tz":-330,"elapsed":6906,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}},"outputId":"53a91f3c-8063-40b8-c6e2-4b49ce719780"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"]}]},{"cell_type":"code","source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import csv\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","import re\n","import string\n","import collections\n","import fse\n","import random\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchnlp.metrics import get_moses_multi_bleu\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.manual_seed(1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2oJ_s6szAf_R","executionInfo":{"status":"ok","timestamp":1696356074380,"user_tz":-330,"elapsed":436,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}},"outputId":"29034abb-7a17-473d-972d-1b1e7e4c50dc"},"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x796d4c47d430>"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["root = \"\"\n","\n","d_pos_path = root+\"train_normal\"\n","d_neg_path = root+\"train_toxic\"\n","\n","#Problem: Data imbalance? -> decoder might be biased towards positive\n","d_pos = pd.read_csv(d_pos_path, delimiter=\"\\0\",header=None).iloc[:10000]\n","d_neg = pd.read_csv(d_neg_path, sep=\"\\0\",header=None).iloc[:10000]\n","d_both = pd.concat((d_pos, d_neg),ignore_index=True)\n","\n","\n","print(d_both.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ff5AdQkwAtFT","executionInfo":{"status":"ok","timestamp":1696356075582,"user_tz":-330,"elapsed":635,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}},"outputId":"3dd7c54b-7a1a-4382-d802-5ac202d99832"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                   0\n","0  just a comment regarding family trusts , they ...\n","1  nor do they conform to the notion that our tit...\n","2  yeah , the pers employees and their pensions f...\n","3  why risk our marine parks and sea life for a f...\n","4  not sure what flavor koolaid you drinking but ...\n"]}]},{"cell_type":"code","source":["d_labels = pd.DataFrame(np.concatenate( ( np.ones((d_pos.size, 1)), np.zeros((d_neg.size, 1)) ) ))"],"metadata":{"id":"n8C4L0RDAuHY","executionInfo":{"status":"ok","timestamp":1696356075583,"user_tz":-330,"elapsed":8,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["d_all = pd.concat((d_pos, d_neg), ignore_index=True)\n","d_all['labels'] = d_labels\n","d_all.columns = ['text', 'labels']"],"metadata":{"id":"iMY0OVkxam8I","executionInfo":{"status":"ok","timestamp":1696356075583,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["cols = d_all.columns.tolist()\n","cols = cols[-1:] + cols[:-1]\n","d_all = d_all[cols]"],"metadata":{"id":"h2hk1Dheaoi4","executionInfo":{"status":"ok","timestamp":1696356075583,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["d_pos=d_pos.iloc[:,0]\n","d_neg=d_neg.iloc[:,0]\n","d_both=d_both.iloc[:,0]"],"metadata":{"id":"jmlewsWXao-O","executionInfo":{"status":"ok","timestamp":1696356075583,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["#Parameters:\n","param_smooth = 1\n","param_threshold = 15\n","param_span = 4\n","\n","param_backoff_limit = 3"],"metadata":{"id":"vnc_c5BkasO_","executionInfo":{"status":"ok","timestamp":1696356075583,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["#ngram has punctuation\n","def has_punctuation(ngram): #damn I'm very proud of making this from scratch lol, looks elegant in one line\n","    return True in [x in string.punctuation for x in ngram]\n","\n","def generate_ngrams(lines, min_length=1, max_length=param_span):\n","#     lines = placeholder + lines\n","    lengths = range(min_length, max_length + 1)\n","    ngrams = {length: [] for length in lengths}\n","    queue = collections.deque(maxlen=max_length)\n","\n","    def add_queue():\n","        current = tuple(queue)\n","        for length in lengths:\n","            if len(current) >= length and not has_punctuation(current[:length]):\n","                ngrams[length].append(current[:length])\n","\n","    short_by = 0\n","    for line in lines:\n","        short_by = max(0, max_length - len(lines))\n","        for word in line.split():\n","            queue.append(word)\n","            if len(queue) >= max_length-short_by:\n","                add_queue()\n","\n","    while len(queue) > min_length:\n","        queue.popleft()\n","        add_queue()\n","    return ngrams\n","\n","#modified from & fixed their error of ngram with # of words < 4: https://gist.github.com/benhoyt/dfafeab26d7c02a52ed17b6229f0cb52\n","def count_ngrams(lines, min_length=1, max_length=param_span):\n","    \"\"\"Iterate through given lines iterator (file object or list of\n","    lines) and return n-gram frequencies. The return value is a dict\n","    mapping the length of the n-gram to a collections.Counter\n","    object of n-gram tuple and number of times that n-gram occurred.\n","    Returned dict includes n-grams of length min_length to max_length.\n","    \"\"\"\n","    lengths = range(min_length, max_length + 1)\n","    ngrams = {length: collections.Counter() for length in lengths}\n","    queue = collections.deque(maxlen=max_length)\n","\n","    # Helper function to add n-grams at start of current queue to dict\n","    def add_queue():\n","        current = tuple(queue)\n","        for length in lengths:\n","            if len(current) >= length and not has_punctuation(current[:length]):\n","                ngrams[length][current[:length]] += 1\n","\n","    # Loop through all lines and words and add n-grams to dict\n","    short_by = 0\n","    for line in lines:\n","        short_by = max(0, max_length - len(lines))\n","        for word in line.split():\n","            queue.append(word)\n","            if len(queue) >= max_length - short_by:\n","                add_queue()\n","\n","    # Make sure we get the n-grams at the tail end of the queue\n","    while len(queue) > min_length:\n","        queue.popleft()\n","        add_queue()\n","\n","    return ngrams"],"metadata":{"id":"QV15uXHOauty","executionInfo":{"status":"ok","timestamp":1696356075583,"user_tz":-330,"elapsed":6,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["#Generate ngram counts for d_pos & d_neg\n","d_pos_ngrams_counts = count_ngrams(d_pos.tolist())\n","d_neg_ngrams_counts = count_ngrams(d_neg.tolist())\n","\n","def get_counts(list1, counted_ngrams):\n","    counts = []\n","    list1_ngrams = generate_ngrams(list1)\n","    list2_counts = counted_ngrams\n","\n","    for length in range(param_span,0, -1):\n","        for v in list1_ngrams[length]:\n","            counts.append([list2_counts[length][v], v])\n","    return np.array(counts)"],"metadata":{"id":"1HuFEbfeawNK","executionInfo":{"status":"ok","timestamp":1696356077978,"user_tz":-330,"elapsed":2401,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["#these are methods that will become useful when extracting attribute markers\n","#why do we need all this? well... that's like 5 hours of debugging...\n","import collections\n","\n","try:\n","    collectionsAbc = collections.abc\n","except AttributeError:\n","    collectionsAbc = collections\n","\n","def flatten(foo):\n","    return list(_flatten(foo))\n","\n","def _flatten(foo):\n","    for x in foo:\n","        if isinstance(x, collectionsAbc.Iterable) and not isinstance(x, str):\n","            for y in _flatten(x):\n","                yield y\n","        else:\n","            yield x\n","\n","def array_to_string(a):\n","    return ' '.join(flatten(a))\n","\n","def is_in_string_array(elements, original): #deprecated, does not take into account sequence order\n","    return np.isin(array_to_string(elements).split(), array_to_string(original).split()).any()\n","\n","def insert_string(string, inserted_string, index):\n","    return string[:index] + inserted_string + string[index:]\n","\n","# modified from https://stackoverflow.com/questions/41752946/replacing-a-character-from-a-certain-index\n","def replace_string(s, newstring, index, nofail=False):\n","    # raise an error if index is outside of the string\n","    if not nofail and index not in range(len(s)):\n","        raise ValueError(\"index outside given string. index:\" + index)\n","\n","    # if not erroring, but the index is still not in the correct range..\n","    if index < 0:  # add it to the beginning\n","        return newstring + s\n","    if index > len(s):  # add it to the end\n","        return s + newstring\n","\n","    # insert the new string between \"slices\" of the original\n","    return s[:index] + newstring + s[index + len(newstring):]"],"metadata":{"id":"Cnrqlus4ayUB","executionInfo":{"status":"ok","timestamp":1696356077978,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["def get_attribute_markers(s, style_src):\n","    sentence = [s]\n","\n","    ngrams = get_counts(sentence, d_pos_ngrams_counts)\n","    if len(ngrams) > 0:\n","        ngrams = ngrams[:,1]\n","\n","    pos_counts = get_counts(sentence, d_pos_ngrams_counts)\n","    if len(pos_counts) > 0:\n","        pos_counts = pos_counts[:,0]\n","\n","    neg_counts = get_counts(sentence, d_neg_ngrams_counts)\n","    if len(neg_counts) > 0:\n","        neg_counts = neg_counts[:,0]\n","\n","\n","    if(style_src):\n","        importances = (pos_counts + param_smooth) / (neg_counts + param_smooth)\n","    else:\n","        importances = (neg_counts + param_smooth) / (pos_counts + param_smooth)\n","\n","    a = []\n","\n","    importances = np.vstack((importances, ngrams)).T\n","    for importance in importances:\n","        if importance[0] > param_threshold and not is_in_string_array(importance[1], a):\n","            a.append(' '.join(importance[1]))\n","    return a"],"metadata":{"id":"glnXn_swa0RU","executionInfo":{"status":"ok","timestamp":1696356077978,"user_tz":-330,"elapsed":3,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["def separate(sentence, style_src):\n","    attributes = get_attribute_markers(sentence, style_src)\n","    c = sentence\n","\n","    replace_indexes = []\n","    for a in attributes:\n","        replace_index = -1\n","        replace_index = c.find(a)\n","        replace_indexes.append(replace_index)\n","        c = c.replace(a, \" \"*len(a))\n","\n","    if len(attributes) == 0:\n","        return {'c': c, 'a': [], 'i': [], 's': sentence}\n","\n","    replace_indexes, attributes = zip(*sorted(zip(replace_indexes, attributes)))\n","    return {'c': c, 'a': attributes, 'i': replace_indexes, 's': sentence}\n","\n","def get_c(sentence, style):\n","    return re.sub(' +', ' ', separate(sentence, style)['c'])\n","\n","def get_a(sentence, style):\n","    a = separate(sentence, style)['a']\n","    if len(a) > 0:\n","        return ' '.join(a)\n","    else:\n","        return \"\""],"metadata":{"id":"IOPjYNJwa1lp","executionInfo":{"status":"ok","timestamp":1696356077978,"user_tz":-330,"elapsed":3,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["# ===== TF-IDF Weighted Word Overlap ===== #\n","# docs pre-processing\n","docs = d_both.tolist()\n","\n","# creating dict_idf = {word: idf}\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_vectorizer=TfidfVectorizer(use_idf=True, stop_words=None)\n","tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)\n","dict_idf = dict(zip(tfidf_vectorizer.get_feature_names_out() , tfidf_vectorizer.idf_))\n","\n","def get_overlap(a, b):\n","#     print(a, b)\n","    a_counter = collections.Counter(a.split())\n","    b_counter = collections.Counter(b.split())\n","    overlap = a_counter & b_counter\n","    return overlap\n","\n","def get_weighted_overlap(a, b):\n","    overlap = get_overlap(a, b)\n","    a_counter = collections.Counter(a.split())\n","    #calculate\n","    weighted_overlap = 0\n","    for word in overlap:\n","\n","        word_tf = a_counter[word]#/len(a.split()) -> commented out cause division by constant value doesn't matter\n","\n","        get_idf = dict_idf.get(word)\n","        word_idf = 1 if get_idf == None else get_idf #get rid of error when idf not in dict\n","\n","        word_tfidf = word_tf*word_idf\n","        weighted_overlap+=overlap[word]*word_tfidf\n","\n","    return weighted_overlap\n","\n","def get_closest_sentence_tfidf(sentence, style_src):\n","    opposite_dataset = d_neg if style_src else d_pos\n","\n","    highest_overlap = 0\n","    closest_sentence = \"\"\n","\n","    min_attribute_markers=len(get_attribute_markers(sentence, style_src))\n","    num_markers = 0\n","\n","    previous_sentences = []\n","    backoff_count = 0\n","    while(num_markers < min_attribute_markers and backoff_count < param_backoff_limit):\n","        for sentence_b in opposite_dataset:\n","            weighted_overlap = get_weighted_overlap(sentence, sentence_b)\n","            if weighted_overlap > highest_overlap and sentence_b not in previous_sentences:\n","                highest_overlap = weighted_overlap\n","                closest_sentence = sentence_b\n","        highest_overlap = 0\n","        backoff_count += 1\n","        previous_sentences.append(closest_sentence)\n","        num_markers = len(get_attribute_markers(closest_sentence, not style_src))\n","\n","    return closest_sentence"],"metadata":{"id":"UGMu0lBJf0CP","executionInfo":{"status":"ok","timestamp":1696356078832,"user_tz":-330,"elapsed":857,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["# Retrieve using tfidf\n","def retrieve(sentence, style_src):\n","    return separate(get_closest_sentence_tfidf(sentence, style_src), not style_src)"],"metadata":{"id":"8_ejJ1_Jf0rp","executionInfo":{"status":"ok","timestamp":1696356078833,"user_tz":-330,"elapsed":14,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":["#RetrieveOnly"],"metadata":{"id":"f0FIPDA1dKsg"}},{"cell_type":"code","source":["\n","\n","\n","SOS_token = 0\n","EOS_token = 1\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"],"metadata":{"id":"y28LJZC-fmH6","executionInfo":{"status":"ok","timestamp":1696356078833,"user_tz":-330,"elapsed":13,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["def get_total_overlap(a, b):\n","    return len(list(get_overlap(a, b).elements()))"],"metadata":{"id":"DK2wiqmgfqcx","executionInfo":{"status":"ok","timestamp":1696356078833,"user_tz":-330,"elapsed":12,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["noise_chance = 0.1\n","\n","def prepareData():\n","    input_lang = Lang(\"input\")\n","    output_lang = Lang(\"output\")\n","\n","    d_pos_a = []\n","\n","    pairs_pos = []\n","    for sentence in d_pos:\n","        c = get_c(sentence, 1)\n","        a = get_a(sentence, 1)\n","        d_pos_a.append(a)\n","        pairs_pos.append([c, a, sentence])\n","\n","    d_neg_a = []\n","\n","    pairs_neg = []\n","    for sentence in d_neg:\n","        c = get_c(sentence, 0)\n","        a = get_a(sentence, 0)\n","        d_neg_a.append(a)\n","        pairs_neg.append([c, a, sentence])\n","\n","    #adding noise for pos\n","    for pair in pairs_pos:\n","        if random.random() < noise_chance:\n","            real_a = pair[1].split()\n","\n","            if(len(real_a) == 0):\n","                continue\n","\n","            for a in d_pos_a:\n","                if(len(a) == 0):\n","                    continue\n","                a = a.split()\n","\n","                overlap = get_total_overlap(' '.join(real_a), ' '.join(a))\n","                if overlap > 0 and ((overlap == len(real_a) - 1 and len(real_a) - len(a) == 1) or (overlap == len(real_a) and len(real_a) - len(a) == -1)):\n","                    real_a = a\n","                    break;\n","            pair[1] = ' '.join(real_a)\n","\n","\n","    #adding noise for neg\n","    for pair in pairs_neg:\n","        if random.random() < noise_chance:\n","            real_a = pair[1].split()\n","            if(len(real_a) == 0):\n","                continue\n","\n","            for a in d_neg_a:\n","                if(len(a) == 0):\n","                    continue\n","                a = a.split()\n","\n","                overlap = get_total_overlap(' '.join(real_a), ' '.join(a))\n","                if overlap > 0 and ((overlap == len(real_a) - 1 and len(real_a) - len(a) == 1) or (overlap == len(real_a) and len(real_a) - len(a) == -1)):\n","                    real_a = a\n","                    break;\n","            pair[1] = ' '.join(real_a)\n","\n","\n","    pairs = np.concatenate((pairs_pos, pairs_neg), 0)\n","\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        input_lang.addSentence(pair[1])\n","        output_lang.addSentence(pair[2])\n","\n","    print(\"Counted words:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","    return input_lang, output_lang, pairs\n","\n","input_lang, output_lang, pairs = prepareData()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_96g9afXfuJv","executionInfo":{"status":"ok","timestamp":1696356101073,"user_tz":-330,"elapsed":22251,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}},"outputId":"2636d8f1-37c1-44d0-cd74-c4bd1eb9604c"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-76-3a1fba66c73a>:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  return np.array(counts)\n"]},{"output_type":"stream","name":"stdout","text":["Counted words:\n","input 22146\n","output 22141\n"]}]},{"cell_type":"code","source":["class Maxout(nn.Module):\n","    def __init__(self, pool_size):\n","        super().__init__()\n","        self._pool_size = pool_size\n","\n","    def forward(self, x):\n","        assert x.shape[1] % self._pool_size == 0, \\\n","            'Wrong input last dim size ({}) for Maxout({})'.format(x.shape[1], self._pool_size)\n","        m, i = x.view(*x.shape[:1], x.shape[1] // self._pool_size, self._pool_size, *x.shape[2:]).max(2)\n","        return m"],"metadata":{"id":"2b4JT6clp9h9","executionInfo":{"status":"ok","timestamp":1696356101074,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, word_vec_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, word_vec_size)\n","        self.gru = nn.GRU(word_vec_size, hidden_size)\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        output = embedded\n","        output, hidden = self.gru(output, hidden)\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"metadata":{"id":"jHTQ2YuPd0xb","executionInfo":{"status":"ok","timestamp":1696356101074,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, word_vec_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(output_size, word_vec_size)\n","        self.gru = nn.GRU(word_vec_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","        self.maxout = Maxout(1)\n","\n","    def forward(self, input, hidden):\n","        output = self.embedding(input).view(1, 1, -1)\n","        output = self.maxout(output)\n","        output, hidden = self.gru(output, hidden)\n","        output = self.softmax(self.out(output[0]))\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"metadata":{"id":"qjfPVUCsd4KF","executionInfo":{"status":"ok","timestamp":1696356101074,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["#Preparing Training Data\n","def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","\n","def tensorFromSentence(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","\n","def tensorsFromPair(pair):\n","    input_c_tensor = tensorFromSentence(input_lang, pair[0])\n","    input_a_tensor = tensorFromSentence(input_lang, pair[1])\n","    target_tensor = tensorFromSentence(output_lang, pair[2])\n","    return (input_c_tensor, input_a_tensor, target_tensor)"],"metadata":{"id":"PoUNz8EXdkWF","executionInfo":{"status":"ok","timestamp":1696356101074,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["#Training\n","MAX_LENGTH = 50\n","\n","teacher_forcing_ratio = 0.5\n","\n","\n","def train(input_c_tensor, input_a_tensor, target_tensor, encoder_c, encoder_a, decoder, encoder_c_optimizer, encoder_a_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n","    encoder_c_hidden = encoder_c.initHidden()\n","    encoder_a_hidden = encoder_a.initHidden()\n","\n","    encoder_a_optimizer.zero_grad()\n","    encoder_c_optimizer.zero_grad()\n","\n","    decoder_optimizer.zero_grad()\n","\n","    input_c_length = input_c_tensor.size(0)\n","    input_a_length = input_a_tensor.size(0)\n","\n","    target_length = target_tensor.size(0)\n","\n","    encoder_c_outputs = torch.zeros(max_length, encoder_c.hidden_size, device=device)\n","    encoder_a_outputs = torch.zeros(max_length, encoder_a.hidden_size, device=device)\n","\n","    loss = 0\n","\n","    for ei in range(input_c_length):\n","        encoder_c_output, encoder_c_hidden = encoder_c(\n","            input_c_tensor[ei], encoder_c_hidden)\n","        encoder_c_outputs[ei] = encoder_c_output[0, 0]\n","\n","    for ei in range(input_a_length):\n","        encoder_a_output, encoder_a_hidden = encoder_a(\n","            input_a_tensor[ei], encoder_a_hidden)\n","        encoder_a_outputs[ei] = encoder_a_output[0, 0]\n","\n","\n","    decoder_input = torch.tensor([[SOS_token]], device=device)\n","\n","    decoder_hidden = torch.cat((encoder_c_hidden, encoder_a_hidden), 2)\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, decoder_hidden)\n","            loss += criterion(decoder_output, target_tensor[di])\n","            decoder_input = target_tensor[di]  # Teacher forcing\n","\n","    else:\n","        # Without teacher forcing: use its own predictions as the next input\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, decoder_hidden)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # detach from history as input\n","\n","            loss += criterion(decoder_output, target_tensor[di])\n","            if decoder_input.item() == EOS_token:\n","                break\n","\n","    loss.backward()\n","\n","    encoder_c_optimizer.step()\n","    encoder_a_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return loss.item() / target_length"],"metadata":{"id":"lmcvKGM6dfaP","executionInfo":{"status":"ok","timestamp":1696356101074,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":89,"outputs":[]},{"cell_type":"code","source":["import time\n","import math\n","\n","def trainIters(encoder_a, encoder_c, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    encoder_c_optimizer = optim.Adadelta(encoder_c.parameters(), lr=learning_rate)\n","    encoder_a_optimizer = optim.Adadelta(encoder_a.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.Adadelta(decoder.parameters(), lr=learning_rate)\n","    training_pairs = [tensorsFromPair(random.choice(pairs))\n","                      for i in range(n_iters)]\n","    criterion = nn.NLLLoss()\n","\n","    for iter in range(1, n_iters + 1):\n","        training_pair = training_pairs[iter - 1]\n","        input_a_tensor = training_pair[0]\n","        input_c_tensor = training_pair[1]\n","\n","        target_tensor = training_pair[2]\n","        loss = train(input_c_tensor, input_a_tensor, target_tensor, encoder_c, encoder_a,\n","                     decoder, encoder_c_optimizer, encoder_c_optimizer, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if iter % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n","                                         iter, iter / n_iters * 100, print_loss_avg))\n","\n","        if iter % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","\n","\n","word_vec_size = 128\n","hidden_size = 512\n","encoder_c = EncoderRNN(input_lang.n_words, word_vec_size, hidden_size).to(device)\n","encoder_a = EncoderRNN(input_lang.n_words, word_vec_size, hidden_size).to(device)\n","\n","decoder = DecoderRNN(hidden_size + hidden_size, word_vec_size, output_lang.n_words).to(device)"],"metadata":{"id":"R_mvPgQ3dsdq","executionInfo":{"status":"ok","timestamp":1696356101775,"user_tz":-330,"elapsed":705,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["def get_c_embedding(encoder_c, c, max_length=MAX_LENGTH):\n","    input_c_tensor = tensorFromSentence(input_lang, c)\n","\n","    input_c_length = input_c_tensor.size()[0]\n","\n","    encoder_c_hidden = encoder_c.initHidden()\n","\n","    encoder_c_outputs = torch.zeros(max_length, encoder_c.hidden_size, device=device)\n","\n","    for ei in range(input_c_length):\n","        encoder_c_output, encoder_c_hidden = encoder_c(input_c_tensor[ei],\n","                                                 encoder_c_hidden)\n","        encoder_c_outputs[ei] += encoder_c_output[0, 0]\n","\n","    return encoder_c_hidden"],"metadata":{"id":"HjQzDN0OdYud","executionInfo":{"status":"ok","timestamp":1696356101775,"user_tz":-330,"elapsed":6,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["def get_euclidean_distance(c, c2):\n","    return torch.dist(get_c_embedding(encoder_c, c), get_c_embedding(encoder_c, c2))"],"metadata":{"id":"SP-n9xsWdOSj","executionInfo":{"status":"ok","timestamp":1696356101775,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":92,"outputs":[]},{"cell_type":"code","source":["def RetrieveOnly(sentence, style_src):\n","    opposite_dataset = d_neg if style_src else d_pos\n","\n","    closest_sentence = \"\"\n","\n","    c_src = get_c(sentence, style_src)\n","\n","    min_distance = -1\n","    for sentence_b in opposite_dataset:\n","        c_tgt = get_c(sentence_b, not style_src)\n","\n","        dist = get_euclidean_distance(c_src, c_tgt)\n","        if min_distance == -1 or dist < min_distance:\n","            min_distance = dist\n","            closest_sentence = sentence_b\n","\n","    return closest_sentence"],"metadata":{"id":"7G6hi7A2dQZC","executionInfo":{"status":"ok","timestamp":1696356101775,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["sentence = \"fuck gay people\"\n","style_src = 0\n","RetrieveOnly(sentence, style_src)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"cjWO3BAOdSkK","executionInfo":{"status":"ok","timestamp":1696356405373,"user_tz":-330,"elapsed":65778,"user":{"displayName":"Shavak Kansal","userId":"01245445376376646906"}},"outputId":"c599dbe6-8f79-4fac-e083-8537b3de745c"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-76-3a1fba66c73a>:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  return np.array(counts)\n"]},{"output_type":"execute_result","data":{"text/plain":["'according to last years census , there are almost DIGIT , DIGIT homes sitting empty in toronto , enough to house almost a quarter million people .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":96}]}]}